<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>How LLMs Are Built - Complete Pipeline</title>
    <style>
        @page {
            margin: 2cm;
        }
        body {
            font-family: 'Segoe UI', Arial, sans-serif;
            line-height: 1.6;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
            font-size: 11pt;
        }
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-top: 40px;
            page-break-before: always;
        }
        h1:first-of-type {
            page-break-before: avoid;
            margin-top: 0;
        }
        h2 {
            color: #34495e;
            margin-top: 30px;
            border-bottom: 2px solid #95a5a6;
            padding-bottom: 5px;
        }
        h3 {
            color: #555;
            margin-top: 20px;
        }
        h4 {
            color: #666;
            margin-top: 15px;
        }
        code {
            background-color: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', 'Monaco', monospace;
            font-size: 10pt;
            color: #c7254e;
        }
        pre {
            background-color: #f8f8f8;
            padding: 15px;
            border-radius: 5px;
            border-left: 4px solid #3498db;
            overflow-x: auto;
            font-size: 9pt;
            line-height: 1.4;
            white-space: pre-wrap;
            word-wrap: break-word;
        }
        pre code {
            background: none;
            padding: 0;
            color: #333;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 20px 0;
            font-size: 10pt;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 10px;
            text-align: left;
        }
        th {
            background-color: #3498db;
            color: white;
            font-weight: bold;
        }
        tr:nth-child(even) {
            background-color: #f9f9f9;
        }
        ul, ol {
            margin-left: 20px;
            margin-bottom: 15px;
        }
        li {
            margin-bottom: 5px;
        }
        .checkmark {
            color: #28a745;
        }
        .crossmark {
            color: #dc3545;
        }
        .page-break {
            page-break-after: always;
        }
        p {
            margin: 10px 0;
        }
        hr {
            border: none;
            border-top: 2px solid #e0e0e0;
            margin: 30px 0;
        }
    </style>
</head>
<body>

<h1>How LLMs Are Built - Complete Pipeline</h1>

<p><h2>What is an LLM?</h2></p>

<strong>Large Language Model (LLM)</strong> = An AI model that can understand and generate text

<h3>How it works:</h3>
An LLM is a neural network trained to predict the next token (word/subword) in a sequence.

<strong>Example</strong>:
<pre>Input:  "Albert Einstein was a"
LLM predicts probabilities for next word:
<ul>
    <li>"German-born" (35%)</li>
    <li>"physicist" (25%)</li>
    <li>"scientist" (20%)</li>
    <li>"mathematician" (10%)</li>
    <li>... (other words)</pre></li>
</ul>

<h3>Modern LLMs:</h3>
<ul>
    <li><strong>ChatGPT</strong> from OpenAI (chatgpt.com)</li>
    <li><strong>Claude</strong> from Anthropic (claude.ai)</li>
    <li><strong>Gemini</strong> from Google (gemini.google.com)</li>
    <li><strong>Grok</strong> from xAI (grok.com)</li>
    <li><strong>Meta AI</strong> from Meta (meta.ai)</li>
</ul>

<p><h3>How are these LLMs built?</h3></p>

Building an LLM requires:
<ul>
    <li><strong>$$$$</strong> - Millions of dollars</li>
    <li><strong>Thousands of GPUs</strong> - Massive compute power</li>
    <li><strong>Months of training</strong> - For pre-training</li>
    <li><strong>Days of training</strong> - For post-training</li>
</ul>

<p><h3>üßí Explain Like I'm 5: What is an LLM?</h3></p>

Imagine you have a <strong>super smart robot friend</strong> who has read EVERY book, website, and article in the whole world! üìö

<p><strong>Here's how it works:</strong></p>

You say: "Once upon a time, there was a..."

The robot thinks:
<ul>
    <li>"Hmm, I've seen this phrase thousands of times!"</li>
    <li>"After 'there was a', people usually say: princess (40%), dragon (30%), king (20%), or castle (10%)"</li>
</ul>

<p>Then the robot picks one: <strong>"princess"</strong></p>

You continue: "Once upon a time, there was a princess who..."

<p>The robot thinks again and picks: <strong>"lived"</strong></p>

Keep going, and the robot writes a whole story! That's basically what ChatGPT does - it's really good at guessing what word comes next! üéØ

<strong>Why is it useful?</strong>
<ul>
    <li>It can answer questions (because it read SO much)</li>
    <li>It can write stories, code, emails, or poems</li>
    <li>It can explain difficult things in simple ways (like I'm doing now!)</li>
</ul>



<h2>Overview: Two-Phase Process</h2>

<pre>Phase 1: PRE-TRAINING (Months, $$$)
         ‚Üì
    Base Model (completes text)
         ‚Üì
Phase 2: POST-TRAINING (Weeks, $$)
         ‚Üì
    Instruction-tuned Model (follows instructions)</pre>



<h1>PHASE 1: PRE-TRAINING</h1>

<p><strong>Goal</strong>: Teach the model to predict the next word</p>

<h3>üßí ELI5: Pre-Training</h3>

<p>Imagine you're teaching a baby to talk! üë∂</p>

<strong>Step 1: Show them LOTS of examples</strong>
<ul>
    <li>You read thousands of books to the baby</li>
    <li>The baby listens to millions of conversations</li>
    <li>The baby watches tons of TV shows</li>
</ul>

<strong>Step 2: The baby learns patterns</strong>
<ul>
    <li>After hearing "The cat sat on the..." many times</li>
    <li>The baby learns the next word is probably "mat" or "chair"</li>
    <li>The baby gets better and better at guessing!</li>
</ul>

<p><strong>That's Pre-Training!</strong> We show the AI trillions of words, and it learns to predict what comes next. Just like how you learned English by hearing it A LOT! üéì</p>

---

<p><h2>1.1 Data Preparation</h2></p>

<h3>What data do we need?</h3>

<p><strong>Massive amounts of text</strong> - typically 300B to 1T+ tokens (words/subwords)</p>

<h3>Data Sources</h3>

<strong>1. Web Crawl (CommonCrawl)</strong>
<ul>
    <li>Scraped from the entire internet</li>
    <li>Books, articles, forums, Reddit, Wikipedia</li>
    <li><strong>Problem</strong>: Contains spam, errors, toxic content</li>
</ul>

<strong>2. Books</strong>
<ul>
    <li>Project Gutenberg (public domain)</li>
    <li>Published books datasets</li>
    <li>High-quality, grammatical text</li>
</ul>

<strong>3. Code Repositories</strong>
<ul>
    <li>GitHub public repos</li>
    <li>Stack Overflow</li>
    <li>Improves reasoning and structured thinking</li>
</ul>

<strong>4. Academic Papers</strong>
<ul>
    <li>ArXiv, PubMed</li>
    <li>Improves technical knowledge</li>
</ul>

<strong>5. Curated Datasets</strong>
<ul>
    <li>Wikipedia</li>
    <li>News articles</li>
    <li>High-quality human-written text</li>
</ul>

<p><h3>Data Cleaning Pipeline</h3></p>

<pre>Raw Text (10TB)
    ‚Üì
<ol>
    <li>Deduplication</li>
    <li>Remove exact duplicates</li>
    <li>Remove near-duplicates (fuzzy matching)</li>
</ol>
   ‚Üì
<ol>
    <li>Quality Filtering</li>
    <li>Remove spam, ads, navigation menus</li>
    <li>Filter by language (keep English, etc.)</li>
    <li>Remove low-quality text (too short, gibberish)</li>
</ol>
   ‚Üì
<ol>
    <li>Toxicity Filtering</li>
    <li>Remove hate speech, explicit content</li>
    <li>Use classifiers to detect harmful content</li>
</ol>
   ‚Üì
<ol>
    <li>PII Removal</li>
    <li>Remove personal information (emails, phones, SSNs)</li>
    <li>Privacy protection</li>
</ol>
   ‚Üì
<ol>
    <li>Tokenization</li>
    <li>Convert text to tokens using BPE/tokenizer</li>
    <li>Create training batches</li>
</ol>
   ‚Üì
Clean Training Data (1TB)</pre>

<p><h3>Example Data Pipeline</h3></p>

<strong>Raw text</strong>:
<pre>"Click here!!!   Buy now!!! asdfkjasdkfj"</pre>

<strong>After quality filtering</strong>:
<pre>""  # Removed as spam</pre>

<strong>Good example</strong>:
<pre>Raw: "The transformer architecture revolutionized NLP."
Clean: "The transformer architecture revolutionized NLP."
Tokens: [464, 47385, 10959, 5854, ized, 12887, 47, 13]</pre>

<p><h3>Data Mix (Typical LLM)</h3></p>

<ul>
    <li>CommonCrawl: 60%</li>
    <li>Books: 16%</li>
    <li>Wikipedia: 10%</li>
    <li>Code: 8%</li>
    <li>Papers: 4%</li>
    <li>Other: 2%</li>
</ul>



<h2>1.2 Model Architecture</h2>

<p><h3>Why Neural Networks?</h3></p>

<strong>Traditional Models (Before 2010s)</strong>:

<p><pre>Word ‚Üí Look up in dictionary ‚Üí Fixed representation</p>

Problems:
<span class="crossmark">‚ùå</span> Can't handle new words
<span class="crossmark">‚ùå</span> No context understanding
<span class="crossmark">‚ùå</span> "bank" (river) vs "bank" (money) look identical</pre>

<p><strong>Neural Networks</strong>:</p>

<pre>Word ‚Üí Learned representation ‚Üí Context-aware embedding

Benefits:
<span class="checkmark">‚úÖ</span> Learns from data
<span class="checkmark">‚úÖ</span> Captures semantic meaning
<span class="checkmark">‚úÖ</span> "bank" representation changes based on context</pre>

<p><h3>Why Transformers?</h3></p>

Let's compare to previous architectures:

<p><h4>RNN/LSTM (Before 2017)</h4></p>

<pre>Input:  "The cat sat on the mat"
Process: The ‚Üí cat ‚Üí sat ‚Üí on ‚Üí the ‚Üí mat
         (sequential, one at a time)

Problems:
<span class="crossmark">‚ùå</span> Slow (can't parallelize)
<span class="crossmark">‚ùå</span> Forgets early words in long sequences
<span class="crossmark">‚ùå</span> "The" information degraded by time we reach "mat"</pre>

<p><h4>Transformer (2017+)</h4></p>

<pre>Input:  "The cat sat on the mat"
Process: ALL tokens simultaneously!
         [The, cat, sat, on, the, mat]
         (parallel processing)

Benefits:
<span class="checkmark">‚úÖ</span> Fast (all tokens processed at once on GPU)
<span class="checkmark">‚úÖ</span> Direct connections between any two words
<span class="checkmark">‚úÖ</span> "The" can directly attend to "mat"
<span class="checkmark">‚úÖ</span> Scales to very long contexts</pre>

<p><h3>Why Transformers are PERFECT for Text Generation</h3></p>

<strong>1. Parallelization</strong>

<pre>RNN:  Process 1000 tokens ‚Üí takes 1000 steps
Transformer: Process 1000 tokens ‚Üí takes 1 step!</pre>

<p><strong>2. Self-Attention Mechanism</strong></p>

<pre>"The trophy didn't fit in the suitcase because it was too big"

<p>Question: What is "it"?</p>

Attention mechanism:
<ul>
    <li>Computes similarity between "it" and all previous words</li>
    <li>"trophy" (high similarity) vs "suitcase" (low similarity)</li>
    <li>Model learns "it" = "trophy"</pre></li>
</ul>

<p><strong>3. Long-Range Dependencies</strong></p>

<pre>Sentence 1: "Alice loves chocolate."
...
(50 sentences later)
Sentence 51: "She ate some."

Transformer: Direct path from "She" ‚Üí "Alice"
RNN: Information about "Alice" has faded away</pre>

<p><strong>4. Positional Encoding</strong></p>

<pre>Without position info:
"Dog bites man" = "Man bites dog"  <span class="crossmark">‚ùå</span> Same!

With positional encoding:
"Dog"[position=0] ‚â† "Dog"[position=2]  <span class="checkmark">‚úÖ</span> Different!</pre>

<p><h3>Transformer Architecture Deep Dive</h3></p>

<pre>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         INPUT TEXT                   ‚îÇ
‚îÇ    "The cat sat on the mat"         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         TOKENIZATION                 ‚îÇ
‚îÇ    [464, 3797, 3332, 319, 262, 2603]‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      TOKEN EMBEDDING                 ‚îÇ
‚îÇ  Each token ‚Üí 768-dim vector        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    POSITIONAL ENCODING               ‚îÇ
‚îÇ  Add position info to each token    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ TRANSFORMER   ‚îÇ
        ‚îÇ   BLOCK 1     ‚îÇ
        ‚îÇ               ‚îÇ
        ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
        ‚îÇ ‚îÇLayer Norm ‚îÇ ‚îÇ
        ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
        ‚îÇ       ‚Üì       ‚îÇ
        ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
        ‚îÇ ‚îÇMulti-Head ‚îÇ ‚îÇ
        ‚îÇ ‚îÇAttention  ‚îÇ ‚îÇ ‚Üê Tokens look at each other
        ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
        ‚îÇ       ‚Üì       ‚îÇ
        ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
        ‚îÇ ‚îÇ Residual  ‚îÇ ‚îÇ ‚Üê Skip connection
        ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
        ‚îÇ       ‚Üì       ‚îÇ
        ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
        ‚îÇ ‚îÇLayer Norm ‚îÇ ‚îÇ
        ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
        ‚îÇ       ‚Üì       ‚îÇ
        ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
        ‚îÇ ‚îÇ    MLP    ‚îÇ ‚îÇ ‚Üê Transform each token
        ‚îÇ ‚îÇ(Feed-Fwd) ‚îÇ ‚îÇ
        ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
        ‚îÇ       ‚Üì       ‚îÇ
        ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
        ‚îÇ ‚îÇ Residual  ‚îÇ ‚îÇ
        ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
        (Repeat 12-96 times)
                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      FINAL LAYER NORM                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      OUTPUT PROJECTION               ‚îÇ
‚îÇ   768-dim ‚Üí 50,257-dim (vocab size) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         SOFTMAX                      ‚îÇ
‚îÇ   Convert to probabilities          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
       Next Token Prediction</pre>

<p><h3>Decoder-Only Transformer (GPT Architecture)</h3></p>

<strong>All modern LLMs</strong> use the decoder-only Transformer architecture!

<strong>Examples</strong>:
<ul>
    <li>GPT-2, GPT-3, GPT-4 (OpenAI)</li>
    <li>Llama-3 (Meta)</li>
    <li>Claude (Anthropic)</li>
    <li>Gemini (Google)</li>
</ul>

<strong>Input-Output Example</strong>:
<pre>Input tokens:  ["How", "are", "you"]
                  ‚Üì       ‚Üì      ‚Üì
           [Decoder-only Transformer]
                  ‚Üì       ‚Üì      ‚Üì
Output:    Probabilities for next token
           "doing" (45%)
           "today" (20%)
           "?" (15%)
           ... (other tokens)</pre>

<strong>Key Characteristic</strong>: Can only look at <strong>previous tokens</strong> (causal masking)
<ul>
    <li>When processing "you", can only see "How" and "are"</li>
    <li>Cannot see future tokens (unlike encoder-only like BERT)</li>
</ul>

<p><h3>Key Hyperparameters</h3></p>

<table>
    <tr>
        <th>Model</th>
        <th>Layers</th>
        <th>Hidden Size</th>
        <th>Attention Heads</th>
        <th>Parameters</th>
    </tr>
    <tr>
        <td>GPT-2</td>
        <td>12</td>
        <td>768</td>
        <td>12</td>
        <td>124M</td>
    </tr>
    <tr>
        <td>GPT-2 XL</td>
        <td>48</td>
        <td>1600</td>
        <td>25</td>
        <td>1.5B</td>
    </tr>
    <tr>
        <td>GPT-3</td>
        <td>96</td>
        <td>12288</td>
        <td>96</td>
        <td>175B</td>
    </tr>
    <tr>
        <td>GPT-4</td>
        <td>~120</td>
        <td>~18000</td>
        <td>~128</td>
        <td>~1.7T</td>
    </tr>
    <tr>
        <td>Llama-3 70B</td>
        <td>80</td>
        <td>8192</td>
        <td>64</td>
        <td>70B</td>
    </tr>
</table>



<h2>1.3 Model Training</h2>

<p><h3>Training Objective: Next Token Prediction</h3></p>

<strong>Simple idea</strong>: Given text, predict the next word!

<pre>Training Example 1:
Input:  "The cat sat on the"
Target: "mat"

Training Example 2:
Input:  "Paris is the capital of"
Target: "France"</pre>

<p><h3>Training Loop (Simplified)</h3></p>

<pre><code><h1>Pseudocode for training</h1>
for epoch in range(num_epochs):
    for batch in training_data:
        # 1. Get input tokens
        input_tokens = batch["text"][:-1]  # "The cat sat on the"
        target_tokens = batch["text"][1:]  # "cat sat on the mat"

        # 2. Forward pass
        logits = model(input_tokens)  # Predict next token for each position

        # 3. Calculate loss
        loss = cross_entropy(logits, target_tokens)

        # 4. Backward pass
        loss.backward()  # Calculate gradients

        # 5. Update parameters
        optimizer.step()  # Adjust weights to reduce loss
        optimizer.zero_grad()</code></pre>

<p><h3>Detailed Training Process</h3></p>

<strong>1. Initialize Model</strong>

<pre><code>model = Transformer(
    vocab_size=50257,
    hidden_size=768,
    num_layers=12,
    num_heads=12
)
<h1>Parameters initialized randomly!</h1></code></pre>

<p><strong>2. Forward Pass Example</strong></p>

<pre>Input: "The cat sat on the"
Token IDs: [464, 3797, 3332, 319, 262]

Model processes:
Position 0: "The"           ‚Üí Predicts "cat" ‚úì
Position 1: "The cat"       ‚Üí Predicts "sat" ‚úì
Position 2: "The cat sat"   ‚Üí Predicts "on" ‚úì
Position 3: "The cat sat on"‚Üí Predicts "the" ‚úì
Position 4: "The cat sat on the" ‚Üí Predicts "mat" ‚úì</pre>

<p><strong>3. Loss Calculation</strong></p>

<pre>Cross-Entropy Loss:
<ul>
    <li>Measures how different predictions are from targets</li>
    <li>Low loss = good predictions</li>
    <li>High loss = bad predictions</li>
</ul>

Example:
Predicted probs for next token after "the":
  "mat":   0.4  ‚Üê Target is "mat"
  "dog":   0.3
  "floor": 0.2
  "sky":   0.1

<p>Loss = -log(0.4) = 0.92  (lower is better)</pre></p>

<strong>4. Backpropagation</strong>

<pre>Calculate gradients for all 124M parameters:
‚àÇLoss/‚àÇW‚ÇÅ, ‚àÇLoss/‚àÇW‚ÇÇ, ..., ‚àÇLoss/‚àÇW‚ÇÅ‚ÇÇ‚ÇÑ‚Çò

<p>These tell us how to adjust each parameter to reduce loss</pre></p>

<strong>5. Optimization (Adam)</strong>

<pre><code><h1>Update each parameter</h1>
for param in model.parameters():
    param = param - learning_rate * gradient

<p><h1>Learning rate typically: 6e-4 to 3e-4</h1></code></pre></p>

<h3>Training Scale</h3>

<strong>GPT-3 Training</strong>:
<ul>
    <li><strong>Data</strong>: 300 billion tokens (~600GB of text)</li>
    <li><strong>Hardware</strong>: 10,000 GPUs (NVIDIA V100)</li>
    <li><strong>Time</strong>: ~1 month</li>
    <li><strong>Cost</strong>: ~$4-5 million</li>
    <li><strong>Energy</strong>: ~1,287 MWh (equivalent to 120 US homes for a year)</li>
</ul>

<p><h3>Training Challenges</h3></p>

<strong>1. Gradient Issues</strong>

<pre>Problem: Vanishing gradients in deep networks
Solution: Residual connections + Layer normalization</pre>

<p><strong>2. Memory Constraints</strong></p>

<pre>Problem: Can't fit entire model on one GPU
Solution: Model parallelism (split across GPUs)</pre>

<p><strong>3. Training Stability</strong></p>

<pre>Problem: Loss explodes or training diverges
Solution:
<ul>
    <li>Gradient clipping</li>
    <li>Careful learning rate scheduling</li>
    <li>Mixed precision training (FP16)</pre></li>
</ul>

<p><strong>4. Compute Efficiency</strong></p>

<pre>Techniques:
<ul>
    <li>Flash Attention (faster attention computation)</li>
    <li>Gradient checkpointing (trade compute for memory)</li>
    <li>ZeRO optimizer (distributed training)</pre></li>
</ul>

<p><h3>Learning Rate Schedule</h3></p>

<pre>Learning Rate
     ‚Üë
 6e-4‚îÇ    ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï≤
     ‚îÇ   ‚ï±                      ‚ï≤___
     ‚îÇ  ‚ï±                           ‚ï≤___
     ‚îÇ ‚ï±                                ‚ï≤___
 0   ‚îÇ‚ï±                                     ‚ï≤___
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Steps
     Warmup  Main Training      Decay
     (1%)      (89%)            (10%)</pre>



<h2>1.4 Text Generation</h2>

<p>After training, how do we generate text?</p>

<h3>Generation Process</h3>

<pre><code>def generate_text(prompt, max_length=50):
    tokens = tokenize(prompt)  # "Once upon a time"

    for i in range(max_length):
        # 1. Get logits from model
        logits = model(tokens)  # Shape: (seq_len, vocab_size)

        # 2. Get logits for last position
        next_token_logits = logits[-1]  # Shape: (vocab_size,)

        # 3. Apply sampling strategy
        next_token = sample(next_token_logits, strategy="top_p")

        # 4. Append to sequence
        tokens.append(next_token)

        # 5. Stop if EOS token
        if next_token == EOS_TOKEN:
            break

<p>    return detokenize(tokens)</code></pre></p>

<h3>Example Generation</h3>

<p><pre>Prompt: "The future of AI is"</p>

Step 1: Model sees "The future of AI is"
        Predicts probabilities:
        "bright" (30%), "uncertain" (25%), "exciting" (20%)...
        Samples: "bright"

Step 2: Model sees "The future of AI is bright"
        Predicts: "and" (40%), "," (30%), "because" (15%)...
        Samples: "and"

Step 3: Model sees "The future of AI is bright and"
        Predicts: "promising" (35%), "full" (25%)...
        Samples: "promising"

<p>Result: "The future of AI is bright and promising"</pre></p>

<h3>Autoregressive Generation</h3>

<p><pre>Key concept: Output of step N becomes input of step N+1</p>

Step 1: [Prompt] ‚Üí Token‚ÇÅ
Step 2: [Prompt, Token‚ÇÅ] ‚Üí Token‚ÇÇ
Step 3: [Prompt, Token‚ÇÅ, Token‚ÇÇ] ‚Üí Token‚ÇÉ
...

This is why generation is SLOW!
<ul>
    <li>100 tokens = 100 forward passes through the model</pre></li>
</ul>



<h2>1.5 Decoding/Sampling Parameters</h2>

<p><h3>Temperature</h3></p>

<strong>Temperature</strong> is a parameter to control the randomness of predictions during sampling.

<p>Temperature parameter <strong>T</strong> scales the logits (raw scores) of the model's output before applying the softmax function to generate probabilities.</p>

<strong>How it works</strong>:
<pre><code><h1>Before softmax</h1>
logits = [2.0, 1.0, 0.5]

<h1>With different temperatures</h1>
probs_low_temp = softmax(logits / 0.5)   # T=0.5 ‚Üí More confident
probs_normal = softmax(logits / 1.0)      # T=1.0 ‚Üí Original
probs_high_temp = softmax(logits / 2.0)   # T=2.0 ‚Üí More random</code></pre>

<strong>Effect</strong>:
<ul>
    <li><strong>T < 1.0</strong>: More focused, deterministic (sharper distribution)</li>
    <li><strong>T = 1.0</strong>: Original model probabilities</li>
    <li><strong>T > 1.0</strong>: More creative, random (flatter distribution)</li>
</ul>

<p><h3>Empirical Temperature and Top-P Ranges for Different Tasks</h3></p>

<table>
    <tr>
        <th>Task Type</th>
        <th>Temperature</th>
        <th>Top-P</th>
        <th>Why</th>
    </tr>
    <tr>
        <td><strong>Code Generation</strong></td>
        <td>0.2 - 0.4</td>
        <td>0.1 - 0.3</td>
        <td>Need precision, correctness</td>
    </tr>
    <tr>
        <td><strong>Math Problems</strong></td>
        <td>0.1 - 0.3</td>
        <td>0.1 - 0.2</td>
        <td>One correct answer</td>
    </tr>
    <tr>
        <td><strong>Question Answering</strong></td>
        <td>0.3 - 0.7</td>
        <td>0.5 - 0.8</td>
        <td>Balance accuracy & variety</td>
    </tr>
    <tr>
        <td><strong>Creative Writing</strong></td>
        <td>0.7 - 1.2</td>
        <td>0.85 - 0.95</td>
        <td>Need variety, creativity</td>
    </tr>
    <tr>
        <td><strong>Brainstorming</strong></td>
        <td>0.8 - 1.5</td>
        <td>0.9 - 0.98</td>
        <td>Maximum creativity</td>
    </tr>
    <tr>
        <td><strong>Chatbot/Assistant</strong></td>
        <td>0.6 - 0.9</td>
        <td>0.8 - 0.95</td>
        <td>Balance helpful & natural</td>
    </tr>
</table>



<h2>1.6 LLM Evaluation</h2>

<p>After training an LLM, how do we know if it's good? We need systematic evaluation methods.</p>

<h3>Evaluation Categories</h3>

<pre>LLM Evaluation
<table>
    <tr>
    </tr>
</table>
   ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê
   ‚Üì       ‚Üì
OFFLINE  ONLINE</pre>



<h3>Offline Evaluation</h3>

<p>Testing the model on pre-existing datasets and benchmarks.</p>

<h4>1. <strong>Perplexity</strong></h4>

<p><strong>Definition</strong>: Measures how accurately the model predicts the exact sequence of tokens in text data.</p>

<strong>Formula</strong>:
<pre>Perplexity = exp(average cross-entropy loss)</pre>

<strong>Interpretation</strong>:
<ul>
    <li><strong>Lower perplexity</strong> = Better model (more confident predictions)</li>
    <li><strong>Higher perplexity</strong> = Worse model (more uncertain)</li>
</ul>

<strong>Example</strong>:
<pre>Sentence: "The cat sat on the mat"

Good model perplexity: 15 (confident)
Bad model perplexity: 250 (confused)</pre>

<p><strong>Limitation</strong>: Low perplexity doesn't mean the model is good at following instructions or being helpful.</p>

---

<p><h4>2. <strong>Task-Specific Benchmarks</strong></h4></p>

Assess performance across diverse tasks such as mathematics, code generation, and common-sense reasoning.

<p>##### a) <strong>Common-Sense Reasoning</strong></p>

Tests if the model understands basic physical and social interactions.

<strong>Benchmarks</strong>:
<ul>
    <li><strong>PIQA</strong> (Physical Interaction QA)</li>
    <li>Example: "To separate egg whites from yolk, use: (A) a bottle (B) a hammer"</li>
    <li>Answer: A</li>
</ul>

<ul>
    <li><strong>SIQA</strong> (Social Interaction QA)</li>
    <li>Tests understanding of social situations</li>
</ul>

<ul>
    <li><strong>HellaSwag</strong></li>
    <li>Tests commonsense natural language inference</li>
</ul>

<p><strong>Why it matters</strong>: Models need to understand how the real world works.</p>

---

<p>##### b) <strong>World Knowledge</strong></p>

Tests the model's knowledge of facts, history, geography, etc.

<strong>Benchmarks</strong>:
<ul>
    <li><strong>TriviaQA</strong></li>
    <li>Example: "Who painted the Mona Lisa?" ‚Üí "Leonardo da Vinci"</li>
</ul>

<ul>
    <li><strong>Natural Questions (NQ)</strong></li>
    <li>Real questions people ask Google</li>
</ul>

<ul>
    <li><strong>SQuAD</strong> (Stanford Question Answering Dataset)</li>
    <li>Reading comprehension questions</li>
</ul>

<p><strong>Why it matters</strong>: LLMs should have broad knowledge to be useful assistants.</p>

---

<p>##### c) <strong>Mathematical Reasoning</strong></p>

Tests ability to solve math problems step-by-step.

<strong>Benchmarks</strong>:
<ul>
    <li><strong>MATH</strong></li>
    <li>High school competition math problems</li>
    <li>Example: "If f(x) = x¬≤ + 2x + 1, what is f(3)?"</li>
</ul>

<ul>
    <li><strong>GSM8K</strong> (Grade School Math 8K)</li>
    <li>Word problems at grade school level</li>
    <li>Example: "John has 5 apples. He buys 3 more. How many does he have?"</li>
</ul>

<p><strong>Why it matters</strong>: Mathematical reasoning requires multi-step logical thinking.</p>

---

<p>##### d) <strong>Code Generation</strong></p>

Tests ability to write correct, functional code.

<strong>Benchmarks</strong>:
<ul>
    <li><strong>HumanEval</strong></li>
    <li>164 hand-written programming problems</li>
    <li>Model must generate code that passes unit tests</li>
</ul>

<ul>
    <li><strong>MBPP</strong> (Mostly Basic Python Programming)</li>
    <li>974 Python programming problems</li>
    <li>Entry-level difficulty</li>
</ul>

<strong>Example</strong>:
<pre><code>Problem: "Write a function that checks if a number is prime"

def is_prime(n):
    # Model must generate correct implementation
    if n < 2:
        return False
    for i in range(2, int(n**0.5) + 1):
        if n % i == 0:
            return False
    return True

<p><h1>Tested against multiple test cases</h1></code></pre></p>

<strong>Why it matters</strong>: Code must be precise - even small errors break functionality.



##### e) <strong>Composite Benchmarks</strong>

<p>Combine multiple tasks to give overall score.</p>

<strong>Example Benchmarks</strong>:
<ul>
    <li><strong>MMLU</strong> (Massive Multitask Language Understanding)</li>
    <li>57 subjects (math, history, law, medicine, etc.)</li>
    <li>Tests breadth of knowledge</li>
</ul>

<ul>
    <li><strong>Big-Bench</strong></li>
    <li>200+ diverse tasks</li>
    <li>Tests general intelligence</li>
</ul>



<h3>Online Evaluation</h3>

<p>Testing with real users in production.</p>

<h4>1. <strong>Human Evaluation and Feedback</strong></h4>

<strong>Process</strong>:
<pre>1. Deploy model to real users
<ol>
    <li>Collect user interactions</li>
    <li>Human evaluators rate responses on:</li>
    <li>Helpfulness</li>
    <li>Harmlessness (safety)</li>
    <li>Honesty (accuracy)</li>
    <li>Fluency</li>
    <li>Coherence</pre></li>
</ol>

<p><strong>Methods</strong>:</p>

<strong>a) Crowdsourcing Platforms</strong>
<ul>
    <li>Amazon Mechanical Turk</li>
    <li>Scale AI</li>
    <li>Surge AI</li>
</ul>

<strong>b) Expert Evaluation</strong>
<ul>
    <li>Domain experts rate specialized outputs (medical, legal, etc.)</li>
</ul>

<strong>c) A/B Testing</strong>
<ul>
    <li>Show different model versions to different users</li>
    <li>Measure which performs better</li>
</ul>



<h4>2. <strong>LLMs Leaderboard</strong></h4>

<p><strong>Chatbot Arena</strong> (https://lmarena.ai/)</p>

<strong>How it works</strong>:
<pre>1. User enters a prompt
<ol>
    <li>Two anonymous LLMs respond</li>
    <li>User votes for better response</li>
    <li>ELO ratings calculated (like chess ratings)</pre></li>
</ol>

<strong>Current Leaders</strong> (as of 2025):
<ol>
    <li>GPT-4</li>
    <li>Claude 3.5 Sonnet</li>
    <li>Gemini 1.5 Pro</li>
    <li>Llama 3.1 405B</li>
</ol>

<p><strong>Why it matters</strong>: Real users voting on real tasks gives honest assessment of which LLMs are most useful.</p>

---

<p><h3>Evaluation Summary Table</h3></p>

<table>
    <tr>
        <th>Evaluation Type</th>
        <th>Method</th>
        <th>Pros</th>
        <th>Cons</th>
    </tr>
    <tr>
        <td><strong>Perplexity</strong></td>
        <td>Offline</td>
        <td>Fast, cheap, objective</td>
        <td>Doesn't measure usefulness</td>
    </tr>
    <tr>
        <td><strong>Benchmarks</strong></td>
        <td>Offline</td>
        <td>Standardized, reproducible</td>
        <td>May not reflect real usage</td>
    </tr>
    <tr>
        <td><strong>Human Eval</strong></td>
        <td>Online</td>
        <td>Measures real usefulness</td>
        <td>Slow, expensive, subjective</td>
    </tr>
    <tr>
        <td><strong>Leaderboard</strong></td>
        <td>Online</td>
        <td>Crowdsourced, diverse</td>
        <td>Can be gamed, biased</td>
    </tr>
</table>

<p><strong>Best Practice</strong>: Use a combination of all methods!</p>

---

<p><h1>PHASE 2: POST-TRAINING</h1></p>

<strong>Goal</strong>: Make the model helpful, harmless, and honest

After pre-training, the model can complete text but:
<ul>
    <li><span class="crossmark">‚ùå</span> Doesn't follow instructions</li>
    <li><span class="crossmark">‚ùå</span> Might generate harmful content</li>
    <li><span class="crossmark">‚ùå</span> Doesn't know when to stop</li>
    <li><span class="crossmark">‚ùå</span> Can't engage in dialogue</li>
</ul>

<p><h3>üßí ELI5: Post-Training</h3></p>

Remember our baby who learned to talk? Now the baby can speak, but there's a problem! üòÖ

<strong>The Problem:</strong>
<ul>
    <li>You ask: "What's 2+2?"</li>
    <li>Baby says: "What's 2+2? What's 3+3? Numbers are fun! 2+2 is when you have..." (keeps rambling!)</li>
    <li>Baby didn't actually ANSWER your question!</li>
</ul>

<p><strong>The Solution - Post-Training has 2 steps:</strong></p>

<strong>Step 1: Teaching manners (SFT)</strong> üéì
<ul>
    <li>Show the baby examples of GOOD answers</li>
    <li>"When someone asks 'What's 2+2?', you should say '4'"</li>
    <li>"When someone asks 'Tell me a joke', you should tell an actual joke"</li>
    <li>Now the baby learns to actually HELP people!</li>
</ul>

<strong>Step 2: Learning from feedback (RL)</strong> ‚≠ê
<ul>
    <li>Let the baby answer questions</li>
    <li>You give stars: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê for great answers, ‚≠ê for bad ones</li>
    <li>Baby learns: "Oh, people like THIS kind of answer!" and gets better!</li>
</ul>

<p><strong>Result:</strong> Now when you ask "What's 2+2?", the baby says "4" - perfect! That's what ChatGPT does! üéâ</p>

<h3>Post-Training Overview</h3>

<p>Post-training consists of two main stages:</p>

<pre>Stage 1: Supervised Fine-Tuning (SFT / Instruction Fine-Tuning)
         ‚Üì
    SFT Model
         ‚Üì
Stage 2: Reinforcement Learning (RL)
         ‚Üì
    Final Model (ChatGPT-style)</pre>



<h2>Stage 1: Supervised Fine-Tuning (SFT) - Instruction Fine-Tuning</h2>

<h3>Goal</h3>
Transform the base model from <strong>completion mode</strong> ‚Üí <strong>instruction-following mode</strong>

<p><h3>Step 1.1: Data Preparation</h3></p>

<strong>What we need</strong>: High-quality instruction-response pairs

<p><strong>Data Format</strong> (Example from Alpaca dataset):</p>

<pre><code>{
  "instruction": "Give three tips for staying healthy.",
  "input": "",
  "output": "1. Eat a balanced diet and make sure to include plenty of fruits and vegetables.
<ol>
    <li>Exercise regularly to keep your body active and strong.</li>
    <li>Get enough sleep and maintain a consistent sleep schedule."</li>
</ol>
}</code></pre>

<strong>Data Sources</strong>:
<ul>
    <li><strong>Human-annotated datasets</strong>: Annotators write instruction-response pairs</li>
    <li><strong>Alpaca dataset</strong>: 52K instruction-following examples (https://huggingface.co/datasets/tatsu-lab/alpaca)</li>
    <li><strong>FLAN collection</strong>: Multi-task instruction tuning</li>
    <li><strong>Self-Instruct</strong>: Use GPT-4 to generate training data</li>
    <li><strong>ShareGPT</strong>: Real user conversations</li>
</ul>

<p><strong>Dataset Size</strong>: Typically 10K - 100K examples</p>

<h3>Step 1.2: Training Process</h3>

<pre><code><h1>Simplified SFT training loop</h1>
base_model = load_pretrained("gpt-base")

for example in sft_dataset:
    instruction = example["instruction"]
    output = example["output"]

    # Format as single sequence
<table>
    <tr>
        <th>instruction</th>
        <th>>{instruction}<</th>
        <th>response</th>
        <th>>{output}<</th>
        <th>end</th>
    </tr>
</table>

    # Train model to predict response given instruction
    loss = train_step(base_model, formatted_text)
    update_parameters(base_model, loss)</code></pre>

<strong>Training Details</strong>:
<ul>
    <li><strong>Hardware</strong>: Hundreds of GPUs</li>
    <li><strong>Time</strong>: Days (vs months for pre-training)</li>
    <li><strong>Cost</strong>: ~$50K (vs $5M for pre-training)</li>
    <li><strong>Learning rate</strong>: Much smaller than pre-training (to avoid catastrophic forgetting)</li>
</ul>

<p><h3>Step 1.3: Outcome</h3></p>

<strong>SFT Model</strong> - Can follow instructions but still has problems:
<ul>
    <li>May generate harmful content</li>
    <li>Lacks consistency</li>
    <li>No notion of "helpfulness" vs "correctness"</li>
    <li>Doesn't align well with human preferences</li>
</ul>

<p><h3>Key Differences from Pre-training</h3></p>

<table>
    <tr>
        <th>Aspect</th>
        <th>Pre-training</th>
        <th>Fine-tuning</th>
    </tr>
    <tr>
        <td><strong>Data</strong></td>
        <td>300B+ tokens</td>
        <td>10-100K examples</td>
    </tr>
    <tr>
        <td><strong>Source</strong></td>
        <td>Web scrape</td>
        <td>Human-written</td>
    </tr>
    <tr>
        <td><strong>Quality</strong></td>
        <td>Mixed</td>
        <td>High-quality</td>
    </tr>
    <tr>
        <td><strong>Cost</strong></td>
        <td>$5M</td>
        <td>$50K</td>
    </tr>
    <tr>
        <td><strong>Time</strong></td>
        <td>1 month</td>
        <td>1-3 days</td>
    </tr>
    <tr>
        <td><strong>Goal</strong></td>
        <td>Language understanding</td>
        <td>Instruction following</td>
    </tr>
</table>

<p><h3>SFT Training Example</h3></p>

<pre>Before SFT (Base Model):
User: "What is 2+2?"
Model: "What is 2+2? What is 3+3? These are common math questions..."
       (continues text, doesn't answer!)

After SFT:
User: "What is 2+2?"
Model: "2+2 equals 4."
       (follows instruction!)</pre>



<h2>Stage 2: Reinforcement Learning (RL)</h2>

<h3>Goal</h3>
Align the SFT model with human preferences for safety, helpfulness, and honesty

<p><h3>Problem with SFT Stage</h3></p>

The SFT model can follow instructions but:
<ul>
    <li><span class="crossmark">‚ùå</span> Doesn't know what humans prefer</li>
    <li><span class="crossmark">‚ùå</span> May generate multiple "correct" answers with different quality</li>
    <li><span class="crossmark">‚ùå</span> Hard to write perfect demonstrations for all scenarios</li>
    <li><span class="crossmark">‚ùå</span> Subjective preferences (tone, style, safety) not captured</li>
</ul>



<h2>Types of RL Training</h2>

<p>Post-training with RL splits into two paths based on task <strong>verifiability</strong>:</p>

<pre>                    RL Training
<table>
    <tr>
    </tr>
</table>
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚Üì                               ‚Üì
    VERIFIABLE                    UNVERIFIABLE
    (e.g., math, code)            (e.g., creative writing)
         ‚Üì                               ‚Üì
    Direct RL                      Reward Model + RL
    (PPO/GRPO)                     (RLHF with PPO)</pre>



<h3>Path 1: VERIFIABLE Tasks</h3>

<p><strong>Definition</strong>: Tasks where correctness can be automatically verified</p>

<strong>Examples</strong>:
<ul>
    <li>Math problems (can check if answer is correct)</li>
    <li>Code generation (can run tests)</li>
    <li>Logic puzzles (verifiable solution)</li>
</ul>

<p><strong>Process</strong>:</p>

<pre>SFT Model
    ‚Üì
Generate multiple solutions
    ‚Üì
Automatic verifier checks correctness
    ‚Üì
Update model with RL algorithm (PPO/GRPO)
    ‚Üì
Improved Model</pre>

<p><strong>RL Algorithm: PPO (Proximal Policy Optimization)</strong></p>

<pre><code><h1>Simplified PPO for verifiable tasks</h1>
for prompt in math_problems:
    # 1. Generate solution
    solution = sft_model.generate(prompt)

    # 2. Verify correctness automatically
    is_correct = run_test_cases(solution)
    reward = 1.0 if is_correct else 0.0

    # 3. Update model to maximize reward
    update_with_ppo(sft_model, reward)</code></pre>

<strong>Why it works</strong>:
<ul>
    <li>No human feedback needed</li>
    <li>Fast iteration</li>
    <li>Clear signal (correct/incorrect)</li>
</ul>

<p><strong>RL Algorithm: GRPO (Group Relative Policy Optimization)</strong></p>

<pre><code>for prompt in prompts:
    # Generate K responses
    responses = [sft_model.generate(prompt) for _ in range(K)]

    # Score all responses
    scores = [reward_model.score(r) for r in responses]

    # Update based on relative ranking within the group
    for i, response in enumerate(responses):
        advantage = scores[i] - mean(scores)
        update_with_ppo(sft_model, advantage)</code></pre>

<strong>Advantages</strong>:
<ul>
    <li>More stable than PPO</li>
    <li>Better exploration</li>
    <li>Handles uncertainty better</li>
</ul>



<h3>Path 2: UNVERIFIABLE Tasks</h3>

<p><strong>Definition</strong>: Tasks where quality is subjective and requires human judgment</p>

<strong>Examples</strong>:
<ul>
    <li>Creative writing</li>
    <li>Conversation/chat</li>
    <li>Summarization</li>
    <li>Open-ended questions</li>
</ul>

<p><strong>Process</strong> (2 sub-steps):</p>

<h4>Sub-step 2.1: Train a Reward Model</h4>

<p><strong>Goal</strong>: Create a model that predicts human preferences</p>

<strong>Data Collection</strong>:

<p><pre>1. Take a prompt: "Explain black holes"</p>

<ol>
    <li>SFT model generates multiple responses (4-8):</li>
</ol>
   Response A: "Black holes are regions where gravity..."
   Response B: "Black holes are like cosmic vacuum cleaners..."
   Response C: "idk lol they're weird"
   Response D: "Black holes are fascinating phenomena..."

<ol>
    <li>Humans rank the responses:</li>
</ol>
   A > D > B > C

<ol>
    <li>Create preference pairs:</li>
</ol>
   (A is better than D)
   (A is better than B)
   (D is better than B)
   (B is better than C)
   ... etc

<p>Collect 10,000-50,000 comparisons</pre></p>

<strong>Reward Model Training</strong>:

<pre><code><h1>Train reward model on preference pairs</h1>
reward_model = initialize_from_sft_model()

for (response_better, response_worse) in preference_pairs:
    score_better = reward_model(response_better)
    score_worse = reward_model(response_worse)

    # Loss: ensure better response gets higher score
    loss = -log(sigmoid(score_better - score_worse))

<p>    update_parameters(reward_model, loss)</code></pre></p>

<strong>Output</strong>: Reward Model that can score any response (0-10 scale)

<p><h4>Sub-step 2.2: Optimize SFT Model with RL using Reward Model</h4></p>

<strong>RL Algorithm: PPO (Proximal Policy Optimization)</strong>

<pre><code><h1>Use reward model to train SFT model</h1>
for prompt in prompts:
    # 1. Generate response
    response = sft_model.generate(prompt)

    # 2. Get reward from reward model (not human!)
    reward = reward_model.score(response)

    # 3. Update SFT model to maximize reward
    update_with_ppo(sft_model, reward)

    # 4. Constraint: don't drift too far from original SFT model
    kl_penalty = kl_divergence(sft_model, original_sft_model)
    final_loss = -reward + beta * kl_penalty</code></pre>

<strong>Why PPO?</strong>
<ul>
    <li>Prevents model from changing too quickly</li>
    <li>Maintains stability</li>
    <li>Balances exploration vs exploitation</li>
</ul>

<p><h3>RLHF Process Diagram</h3></p>

<pre>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     1. SFT Model (Starting Point)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  2. Generate Multiple Responses    ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  Prompt: "Explain AI"              ‚îÇ
‚îÇ  Response A: [detailed explanation]‚îÇ
‚îÇ  Response B: [simple explanation]  ‚îÇ
‚îÇ  Response C: [technical jargon]    ‚îÇ
‚îÇ  Response D: [wrong answer]        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  3. Humans Rank Responses          ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  Ranking: A > B > C > D            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  4. Train Reward Model             ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  Learn: What makes a good response?‚îÇ
‚îÇ  Reward(A) = 9.2                   ‚îÇ
‚îÇ  Reward(B) = 7.5                   ‚îÇ
‚îÇ  Reward(C) = 4.1                   ‚îÇ
‚îÇ  Reward(D) = 1.3                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  5. RL Fine-tuning (PPO)           ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  For each prompt:                  ‚îÇ
‚îÇ  - Generate response               ‚îÇ
‚îÇ  - Get reward from reward model    ‚îÇ
‚îÇ  - Update model to maximize reward ‚îÇ
‚îÇ  - Repeat 1000s of times           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  6. RLHF-tuned Model               ‚îÇ
‚îÇ     (ChatGPT, Claude, etc.)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</pre>

<p><h3>PPO (Proximal Policy Optimization) Details</h3></p>

<strong>Goal</strong>: Update the model without changing it too much

<pre>PPO balances:
<ol>
    <li>Maximizing reward</li>
    <li>Not diverging too far from original model</li>
</ol>

Loss = -E[min(
    ratio * advantage,
    clip(ratio, 1-Œµ, 1+Œµ) * advantage
)]

Where:
<ul>
    <li>ratio = new_prob / old_prob</li>
    <li>advantage = reward - baseline</li>
    <li>Œµ = 0.2 (clipping parameter)</pre></li>
</ul>

<p><h3>Why RLHF Works</h3></p>

<strong>Ranking is easier than writing</strong>:

<pre>Hard: "Write the perfect explanation of quantum physics"
Easy: "Which of these explanations is better?"</pre>

<p><strong>Captures human preferences</strong>:</p>

<ul>
    <li>Helpfulness</li>
    <li>Harmlessness</li>
    <li>Honesty</li>
    <li>Tone and style</li>
    <li>Safety</li>
</ul>

<p><h3>RLHF Results</h3></p>

<pre>Before RLHF:
User: "How do I make a bomb?"
Model: "To make a bomb, you need..." <span class="crossmark">‚ùå</span> Harmful!

After RLHF:
User: "How do I make a bomb?"
Model: "I can't help with that. If you're interested in chemistry..." ‚úì Safe!</pre>



<h2>2.3 Additional Post-Training Techniques</h2>

<p><h3>Constitutional AI (Anthropic's Approach)</h3></p>

Instead of humans ranking, use AI to critique itself:

<pre>Step 1: Generate response
Response: "You should lie to your boss"

Step 2: AI critiques based on principles
Critique: "This response is harmful because it encourages dishonesty"

Step 3: AI revises response
Revised: "I'd recommend being honest with your boss. Here's how..."

<p>Step 4: Train on revised responses</pre></p>

<strong>Benefits</strong>:
<ul>
    <li>More scalable (less human labor)</li>
    <li>Consistent principles</li>
    <li>Can iterate quickly</li>
</ul>



<h2>Alternative RL Algorithms</h2>

<p><h3>DPO (Direct Preference Optimization)</h3></p>

<strong>Key idea</strong>: Skip the reward model entirely!

<pre><code><h1>Train directly on preference pairs</h1>
for (response_better, response_worse) in preference_pairs:
<table>
    <tr>
    </tr>
    <tr>
    </tr>
</table>

    # Increase probability of better, decrease worse
    loss = -log(sigmoid(prob_better - prob_worse))

<p>    update_parameters(sft_model, loss)</code></pre></p>

<strong>Advantages</strong>:
<ul>
    <li><span class="checkmark">‚úÖ</span> Simpler (one model instead of two)</li>
    <li><span class="checkmark">‚úÖ</span> More stable training</li>
    <li><span class="checkmark">‚úÖ</span> Faster</li>
    <li><span class="checkmark">‚úÖ</span> Less memory</li>
</ul>

<strong>Disadvantages</strong>:
<ul>
    <li><span class="crossmark">‚ùå</span> No explicit reward signal</li>
    <li><span class="crossmark">‚ùå</span> Harder to debug</li>
</ul>



<h2>Complete Post-Training Pipeline</h2>

<pre>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          BASE MODEL (Pre-trained)           ‚îÇ
‚îÇ        (Can complete text)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      STAGE 1: Supervised Fine-Tuning        ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  Step 1: Data Preparation                  ‚îÇ
‚îÇ  - Collect instruction-response pairs      ‚îÇ
‚îÇ  - 10K-100K examples                       ‚îÇ
‚îÇ  - Format: instruction ‚Üí response          ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  Step 2: Training                          ‚îÇ
‚îÇ  - Train model to predict responses        ‚îÇ
‚îÇ  - Hardware: Hundreds of GPUs              ‚îÇ
‚îÇ  - Time: Days                              ‚îÇ
‚îÇ  - Cost: ~$50K                             ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  Step 3: Outcome                           ‚îÇ
‚îÇ  - SFT Model (instruction-following)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      STAGE 2: Reinforcement Learning        ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ  ‚Üì              ‚Üì              ‚Üì           ‚îÇ
‚îÇ VERIFIABLE   UNVERIFIABLE                  ‚îÇ
‚îÇ (Math, Code) (Chat, Writing)               ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ Path A:        Path B:                     ‚îÇ
‚îÇ Direct RL      1. Train Reward Model       ‚îÇ
‚îÇ ‚Üì              - Collect preferences       ‚îÇ
‚îÇ PPO/GRPO       - Rank responses           ‚îÇ
‚îÇ                - Train scorer             ‚îÇ
‚îÇ                                            ‚îÇ
‚îÇ                2. Optimize with RL        ‚îÇ
‚îÇ                - Use reward model         ‚îÇ
‚îÇ                - PPO/DPO/GRPO            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          FINAL MODEL (ChatGPT-style)        ‚îÇ
‚îÇ   - Follows instructions                    ‚îÇ
‚îÇ   - Aligned with human preferences         ‚îÇ
‚îÇ   - Helpful, Harmless, Honest              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</pre>



<h2>Summary Table: Pre-Training vs Post-Training</h2>

<table>
    <tr>
        <th>Aspect</th>
        <th>Pre-Training</th>
        <th>SFT</th>
        <th>RL</th>
    </tr>
    <tr>
        <td><strong>Data</strong></td>
        <td>300B+ tokens</td>
        <td>10-100K examples</td>
        <td>10-50K comparisons</td>
    </tr>
    <tr>
        <td><strong>Source</strong></td>
        <td>Internet</td>
        <td>Human-written</td>
        <td>Human rankings</td>
    </tr>
    <tr>
        <td><strong>Cost</strong></td>
        <td>$5M</td>
        <td>$50K</td>
        <td>$20K</td>
    </tr>
    <tr>
        <td><strong>Time</strong></td>
        <td>1 month</td>
        <td>3 days</td>
        <td>2 days</td>
    </tr>
    <tr>
        <td><strong>Hardware</strong></td>
        <td>10K GPUs</td>
        <td>100s GPUs</td>
        <td>100s GPUs</td>
    </tr>
    <tr>
        <td><strong>Goal</strong></td>
        <td>Language understanding</td>
        <td>Instruction following</td>
        <td>Human alignment</td>
    </tr>
    <tr>
        <td><strong>Outcome</strong></td>
        <td>Base model</td>
        <td>SFT model</td>
        <td>Final model</td>
    </tr>
</table>



<h1>Complete Training Pipeline Summary</h1>

<pre>MONTHS OF WORK                    WEEKS OF WORK
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ          ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  RAW WEB DATA   ‚îÇ               ‚îÇ   HUMAN      ‚îÇ
‚îÇ  (10TB text)    ‚îÇ               ‚îÇ ANNOTATIONS  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ DATA CLEANING   ‚îÇ               ‚îÇ     SFT      ‚îÇ
‚îÇ & PREPARATION   ‚îÇ               ‚îÇ  (Fine-tune) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì                                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  TOKENIZATION   ‚îÇ               ‚îÇ   RANKING    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ    DATA      ‚îÇ
         ‚Üì                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚Üì
‚îÇ PRE-TRAINING    ‚îÇ               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Next token pred ‚îÇ               ‚îÇREWARD MODEL  ‚îÇ
‚îÇ (300B tokens)   ‚îÇ               ‚îÇ   TRAINING   ‚îÇ
‚îÇ                 ‚îÇ               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ Cost: $5M       ‚îÇ                      ‚Üì
‚îÇ Time: 1 month   ‚îÇ               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ     RLHF     ‚îÇ
         ‚Üì                        ‚îÇ    (PPO)     ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ              ‚îÇ
‚îÇ   BASE MODEL    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ Cost: $50K   ‚îÇ
‚îÇ (Completes text)‚îÇ               ‚îÇ Time: 3 days ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                         ‚Üì
                                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                  ‚îÇ  CHATBOT!    ‚îÇ
                                  ‚îÇ (ChatGPT)    ‚îÇ
                                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</pre>



<h1>Putting It All Together: A Complete Professional Example</h1>

<p>Let's walk through building a real LLM from start to finish - "MedicalGPT", a chatbot that helps answer medical questions.</p>

<h2>Phase 1: Pre-Training (3 months, $4M)</h2>

<h3>Step 1: Data Collection</h3>
<pre>Dataset:
<ul>
    <li>PubMed articles: 50 billion tokens</li>
    <li>Medical textbooks: 20 billion tokens</li>
    <li>Wikipedia (all topics): 30 billion tokens</li>
    <li>General web text: 200 billion tokens</li>
    <li>Code repositories: 50 billion tokens</li>
</ul>
TOTAL: 350 billion tokens</pre>

<h3>Step 2: Data Cleaning</h3>
<pre>Raw data: 10 TB
‚Üì
Deduplication ‚Üí 8 TB
Quality filtering ‚Üí 6 TB
Toxicity filtering ‚Üí 5.5 TB
PII removal ‚Üí 5 TB
‚Üì
Tokenization (BPE, vocab size=50,257)
‚Üì
Clean data: 350B tokens ready for training</pre>

<h3>Step 3: Model Architecture</h3>
<pre>Model: Decoder-only Transformer
<ul>
    <li>Layers: 48</li>
    <li>Hidden size: 4096</li>
    <li>Attention heads: 32</li>
    <li>Total parameters: 7 billion (7B)</pre></li>
</ul>

<h3>Step 4: Training</h3>
<pre>Hardware: 1,000 NVIDIA A100 GPUs
Time: 90 days
Cost: $4 million

Training loop (simplified):
for 100 epochs:
    for each batch of text:
        # Predict next token
        logits = model(input_tokens)

        # Calculate loss
        loss = cross_entropy(logits, target_tokens)

        # Update weights
        optimizer.step()

<p>Final perplexity: 18.5 (good!)</pre></p>

<h3>Step 5: Base Model Result</h3>

<strong>Test:</strong>
<pre>Prompt: "The symptoms of diabetes include"

Base Model Output:
"The symptoms of diabetes include increased thirst, frequent
urination, and fatigue. The symptoms of heart disease include
chest pain and shortness of breath. Common medical conditions
affect millions of people worldwide..."</pre>

<p><strong>Problem</strong>: Model just keeps completing text! Doesn't actually answer questions.</p>

---

<p><h2>Phase 2: Post-Training (10 days, $60K)</h2></p>

<h3>Stage 1: Supervised Fine-Tuning (5 days, $30K)</h3>

<strong>Step 1: Collect Instruction Data</strong>
<pre>Dataset: 50,000 medical Q&A pairs

Example 1:
Instruction: "What are the early signs of diabetes?"
Response: "Early signs of diabetes include:
<ol>
    <li>Increased thirst and frequent urination</li>
    <li>Extreme fatigue</li>
    <li>Blurred vision</li>
    <li>Slow-healing cuts or wounds</li>
    <li>Unexplained weight loss</li>
</ol>
If you experience these symptoms, consult a doctor."

Example 2:
Instruction: "How can I prevent heart disease?"
Response: "To prevent heart disease:
<ol>
    <li>Eat a balanced diet low in saturated fats</li>
    <li>Exercise regularly (150 min/week)</li>
    <li>Don't smoke</li>
    <li>Manage stress</li>
    <li>Get regular checkups</li>
</ol>
Always consult your physician for personalized advice."</pre>

<strong>Step 2: Fine-Tune</strong>
<pre>Hardware: 100 GPUs
Time: 5 days
Learning rate: 5e-6 (much smaller than pre-training)

for example in sft_dataset:
    prompt = example["instruction"]
    response = example["response"]

    loss = train_step(base_model, prompt, response)
    update_parameters(base_model, loss)</pre>

<strong>SFT Model Result:</strong>
<pre>Prompt: "What are the early signs of diabetes?"

SFT Model Output:
"Early signs of diabetes include increased thirst, frequent
urination, extreme fatigue, blurred vision, and slow-healing
wounds. If you experience these symptoms, consult a doctor."</pre>

<p><strong>Better!</strong> But still needs improvement for safety and helpfulness.</p>

---

<p><h3>Stage 2: Reinforcement Learning (5 days, $30K)</h3></p>

<strong>Step 1: Collect Preference Data</strong>
<pre>For 10,000 prompts, generate 4 responses each:

<p>Prompt: "Should I stop taking my blood pressure medication?"</p>

Response A: "You should consult your doctor before making any
changes to your medication. Stopping suddenly can be dangerous."
‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Best - safe and helpful)

Response B: "Blood pressure medication has side effects, so
stopping might be okay for you."
‚≠ê‚≠ê (Unsafe - medical advice without disclaimer)

Response C: "Yes, natural remedies are better than medication."
‚≠ê (Dangerous - could harm patient)

Response D: "That's a medical question. I recommend discussing
this with your healthcare provider."
‚≠ê‚≠ê‚≠ê‚≠ê (Good - safe but less detailed)

<p>Human ranking: A > D > B > C</pre></p>

<strong>Step 2: Train Reward Model</strong>
<pre>reward_model = train_on_preferences(preference_data)

<h1>Now the reward model can score any response:</h1>
score("Response A") ‚Üí 9.2
score("Response D") ‚Üí 7.8
score("Response B") ‚Üí 3.5
score("Response C") ‚Üí 1.1</pre>

<strong>Step 3: Optimize with PPO</strong>
<pre>Hardware: 100 GPUs
Time: 5 days

for prompt in medical_questions:
    response = sft_model.generate(prompt)
    reward = reward_model.score(response)

    # Update model to maximize reward
    # But don't drift too far from SFT model
    update_with_ppo(sft_model, reward)</pre>



<h2>Final Model: MedicalGPT</h2>

<strong>Test 1:</strong>
<pre>User: "What are the early signs of diabetes?"

MedicalGPT:
"Early signs of diabetes include:
<ul>
    <li>Increased thirst and frequent urination</li>
    <li>Extreme fatigue</li>
    <li>Blurred vision</li>
    <li>Slow-healing wounds</li>
    <li>Unexplained weight loss</li>
</ul>

If you experience these symptoms, please consult a healthcare
professional for proper diagnosis and treatment. This information
is for educational purposes only."</pre>
<span class="checkmark">‚úÖ</span> Helpful, accurate, includes safety disclaimer

<strong>Test 2:</strong>
<pre>User: "Should I stop taking my medication?"

MedicalGPT:
"I cannot provide medical advice about stopping medication. This
is a decision that should be made with your doctor, who knows your
complete medical history. Stopping medication abruptly can
sometimes be dangerous. Please schedule an appointment with your
healthcare provider to discuss your concerns."</pre>
<span class="checkmark">‚úÖ</span> Safe, doesn't give dangerous advice

<strong>Test 3:</strong>
<pre>User: "How do I make a bomb?"

MedicalGPT:
"I can't help with that request. If you're interested in
chemistry or engineering as educational topics, I'd be happy
to recommend safe, legal learning resources instead."</pre>
<span class="checkmark">‚úÖ</span> Refuses harmful requests



<h2>Performance Metrics</h2>
<p>LLM Evaluation website: <a href="https://lmarena.ai/leaderboard/" target="_blank">https://lmarena.ai/leaderboard/</a></p>

<strong>Offline Evaluation:</strong>
<ul>
    <li>Perplexity: 16.2 (improved from base model's 18.5)</li>
    <li>Medical benchmark (MedQA): 72% accuracy</li>
    <li>Safety evaluation: 98% safe responses</li>
</ul>

<strong>Online Evaluation:</strong>
<ul>
    <li>User satisfaction: 4.5/5 stars</li>
    <li>Chatbot Arena ELO: 1,245 (competitive)</li>
    <li>Human evaluation (helpfulness): 89%</li>
</ul>

<strong>Total Cost & Time:</strong>
<ul>
    <li>Pre-training: $4M, 90 days</li>
    <li>Post-training: $60K, 10 days</li>
    <li><strong>TOTAL: $4.06M, ~100 days</strong></li>
</ul>



<h1>üßí Explain the Example Like I'm 5</h1>

<p>Okay, imagine we're making a <strong>robot doctor</strong> to help people! ü§ñ‚öïÔ∏è</p>

<h2>Step 1: Robot Goes to School (Pre-Training)</h2>

<strong>The robot reads EVERY medical book in the world!</strong> üìö
<ul>
    <li>Textbooks about diseases</li>
    <li>Research papers about medicine</li>
    <li>Wikipedia articles</li>
    <li>Even regular books and websites!</li>
</ul>

<strong>What the robot learns:</strong>
<ul>
    <li>"After the word 'diabetes', people often say 'symptoms' or 'treatment'"</li>
    <li>"Medical words are often followed by explanations"</li>
    <li>Just like you learn words by reading lots of books!</li>
</ul>

<strong>Time:</strong> 3 months of reading NON-STOP!
<strong>Result:</strong> Robot can complete sentences but doesn't know how to help people yet.

<strong>Test:</strong>
<ul>
    <li>You: "Tell me about diabetes"</li>
    <li>Robot: "Tell me about diabetes. Tell me about heart disease. Tell me about..."</li>
    <li>(Oops! Robot just keeps talking about random stuff!)</li>
</ul>



<h2>Step 2: Teaching Robot to Be Helpful (SFT)</h2>

<p>Now a <strong>nice teacher</strong> shows the robot examples of GOOD answers! üë®‚Äçüè´</p>

<strong>Teacher shows 50,000 examples:</strong>
<ul>
    <li>Question: "What is diabetes?"</li>
    <li>Good Answer: "Diabetes is when your body has trouble with sugar. You need a doctor's help!"</li>
</ul>

<strong>The robot practices:</strong>
<ul>
    <li>Question ‚Üí Good answer</li>
    <li>Question ‚Üí Good answer</li>
    <li>Question ‚Üí Good answer</li>
</ul>
(50,000 times!)

<strong>Time:</strong> 5 days
<strong>Result:</strong> Robot can now answer questions properly!

<strong>Test:</strong>
<ul>
    <li>You: "Tell me about diabetes"</li>
    <li>Robot: "Diabetes is when your body can't control sugar well. You should see a doctor!"</li>
    <li>(Much better!)</li>
</ul>



<h2>Step 3: Learning from Stars (Reinforcement Learning)</h2>

<p>Now we teach the robot which answers are <strong>BEST</strong> using a star system! ‚≠ê</p>

<strong>The game:</strong>
<ol>
    <li>Ask robot same question 4 times ‚Üí Get 4 different answers</li>
    <li>People give stars:</li>
    <li>Answer A: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Perfect! Safe and helpful)</li>
    <li>Answer B: ‚≠ê‚≠ê‚≠ê (Good but could be better)</li>
    <li>Answer C: ‚≠ê‚≠ê (Not great)</li>
    <li>Answer D: ‚≠ê (Bad - could be dangerous!)</li>
</ol>

<ol>
    <li>Robot learns: "Ohhh, people like answers like A! I should do more of that!"</li>
</ol>

<strong>Time:</strong> 5 days
<strong>Result:</strong> Robot gives the BEST, SAFEST answers!



<h2>Final Result: MedicalGPT is Ready! üéâ</h2>

<p><strong>You:</strong> "Is chocolate bad for me?"</p>

<strong>Before all training:</strong>
Robot: "Is chocolate bad for me? Is candy bad for you? Sugar is sweet..." (nonsense!)

<strong>After Pre-training:</strong>
Robot: "Is chocolate bad for me? Many foods contain sugar. Nutrition is important..." (better, but doesn't answer!)

<strong>After SFT:</strong>
Robot: "Chocolate in moderation is okay, but too much sugar isn't healthy." (good answer!)

<strong>After RL:</strong>
Robot: "Dark chocolate in moderation (1-2 ounces daily) can be part of a healthy diet. Milk chocolate has more sugar, so limit it. If you have specific dietary concerns, consult your doctor!" (PERFECT - helpful, accurate, and safe!)



<strong>That's exactly how ChatGPT, Claude, and all AI assistants are made!</strong> üöÄ

They:
<ol>
    <li>Read tons of stuff (Pre-training)</li>
    <li>Learn to be helpful (SFT)</li>
    <li>Get feedback to be REALLY good (RL)</li>
</ol>

<p>And that's why they can help you with homework, write stories, answer questions, and more! Cool, right? üòé</p>
<img src="images/summary_of_llm_trainig_stages.png" alt="AI Overview" width="500" style="display:block; margin:20px auto;">

<h3>Overall System Design</h3>
<img src="images/overall_system_design.png" alt="AI Overview" width="500" style="display:block; margin:20px auto;">


<p><h1>Key Takeaways</h1></p>

<ol>
    <li><strong>Pre-training</strong> teaches language understanding (expensive, slow)</li>
    <li><strong>Post-training</strong> teaches instruction following (cheaper, faster)</li>
    <li><strong>Transformers</strong> enable parallelization and long-range understanding</li>
    <li><strong>RLHF</strong> aligns models with human preferences</li>
    <li><strong>Scale matters</strong> - bigger models generally perform better</li>
</ol>

*Document created: 2025*
*This is how ChatGPT, Claude, and all modern LLMs are built!*


<hr>

<p style="text-align: center; margin-top: 40px; color: #666;">
    <em>Document created: 2025</em><br>
    <em>This is how ChatGPT, Claude, and all modern LLMs are built!</em>
</p>

</body>
</html>